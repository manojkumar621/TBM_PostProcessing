{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install scikit-image\n",
    "# ! pip install tensorboardX\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", (UserWarning, FutureWarning))\n",
    "from utils.hparams import HParam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from dataset import dataloader\n",
    "from utils import metrics\n",
    "from core.res_unet import ResUnet\n",
    "from core.res_unet_plus import ResUnetPlusPlus\n",
    "from utils.logger import MyWriter\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import skimage\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def main(hp, name):\n",
    "    \n",
    "    output_dir = hp.pred\n",
    "\n",
    "    os.makedirs(\"{}/{}\".format(hp.log, name), exist_ok=True)\n",
    "    writer = MyWriter(\"{}/{}\".format(hp.log, name))\n",
    "    # get model\n",
    "\n",
    "    if hp.RESNET_PLUS_PLUS:\n",
    "        model = ResUnetPlusPlus(3).cuda()\n",
    "    else:\n",
    "        model = ResUnet(3, 64).cuda()\n",
    "\n",
    "    checkpoint = torch.load(hp.checkpoints)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(\n",
    "        \"=> loaded checkpoint '{}' (epoch {})\".format(\n",
    "            hp.checkpoints, checkpoint[\"epoch\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mass_dataset_val = dataloader.ImageDataset(\n",
    "        hp, False, transform=transforms.Compose([dataloader.ToTensorTarget()])\n",
    "    )\n",
    "    \n",
    "    # creating loaders\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        mass_dataset_val, batch_size=1, num_workers=2, shuffle=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    print(type(val_dataloader))\n",
    "    \n",
    "    for idx, data in enumerate(tqdm(val_dataloader, desc=\"validation\")):\n",
    "        print(data)\n",
    "\n",
    "        inputs = data[\"sat_img\"].cuda()\n",
    "\n",
    "        prob_map = model(inputs) # last activation was a sigmoid\n",
    "        outputs = (prob_map > 0.3).float()\n",
    "        \n",
    "        print(outputs.shape)\n",
    "        image_name = mass_dataset_val.getPath(idx).split(\"/\")[-1]\n",
    "        print(image_name)\n",
    "        \n",
    "        img = outputs[0] #torch.Size([3,28,28]\n",
    "        \n",
    "        save_image(img, f'{output_dir}/pred_img_{idx}_{image_name}')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    args = {\n",
    "        \"name\": \"ResUnetTrain3\",\n",
    "        \"config\": \"configs/default3.yaml\",\n",
    "        \"resume\": \"\"\n",
    "    }\n",
    "    \n",
    "    class Struct:\n",
    "        def __init__(self, entries):\n",
    "            self.__dict__.update(entries)\n",
    "            \n",
    "    args = Struct(args)\n",
    "    \n",
    "    hp = HParam(args.config)\n",
    "    with open(args.config, \"r\") as f:\n",
    "        hp_str = \"\".join(f.readlines())\n",
    "\n",
    "    main(hp, name=args.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint '/home/manojkumargalla/PostProcess/models/ResUnetTrain5_checkpoint_1000.pt' (epoch 45)\n",
      "['S-1905-018731_PAS_2of2', 'S-1904-007293_PAS_1of2', 'S-1905-017738_PAS_1of2', 'S-2106-003588_PAS_1of2', 'S-1908-010066_PAS_1of2', 'S-2001-005357_PAS_1of2', 'S-2103-004857_PAS_2of2', 'S-1909-007149_PAS_1of2', 'S-1910-000089_PAS_2of2', '18-162_PAS_4of6']\n",
      "There are 10 image folders\n",
      "Found 548 images\n",
      "working on S-1905-018731_PAS_2of2\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation:   0%|          | 0/548 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sat_img': tensor([[[[0.7922, 0.7294, 0.6941,  ..., 0.6627, 0.6980, 0.7255],\n",
      "          [0.6824, 0.6353, 0.6471,  ..., 0.5922, 0.6549, 0.6667],\n",
      "          [0.7373, 0.7216, 0.7373,  ..., 0.5647, 0.5843, 0.5804],\n",
      "          ...,\n",
      "          [0.4706, 0.4549, 0.3373,  ..., 0.7333, 0.7020, 0.6863],\n",
      "          [0.4314, 0.4510, 0.4196,  ..., 0.7333, 0.7098, 0.6667],\n",
      "          [0.4431, 0.4510, 0.4157,  ..., 0.6980, 0.6627, 0.6588]],\n",
      "\n",
      "         [[0.6784, 0.6157, 0.5373,  ..., 0.3490, 0.4275, 0.4549],\n",
      "          [0.6039, 0.5569, 0.5216,  ..., 0.3216, 0.4000, 0.4118],\n",
      "          [0.6941, 0.6784, 0.6627,  ..., 0.3529, 0.3569, 0.3529],\n",
      "          ...,\n",
      "          [0.1647, 0.1490, 0.1765,  ..., 0.6078, 0.5725, 0.5569],\n",
      "          [0.1412, 0.1608, 0.2078,  ..., 0.6000, 0.5804, 0.5373],\n",
      "          [0.1490, 0.1569, 0.1843,  ..., 0.5647, 0.5255, 0.5216]],\n",
      "\n",
      "         [[0.7686, 0.7059, 0.7843,  ..., 0.6706, 0.7137, 0.7412],\n",
      "          [0.6667, 0.6196, 0.7333,  ..., 0.6471, 0.6941, 0.7059],\n",
      "          [0.7373, 0.7216, 0.8196,  ..., 0.6627, 0.6510, 0.6471],\n",
      "          ...,\n",
      "          [0.4745, 0.4588, 0.5490,  ..., 0.7569, 0.7098, 0.6941],\n",
      "          [0.4549, 0.4745, 0.5569,  ..., 0.7608, 0.7176, 0.6745],\n",
      "          [0.4431, 0.4510, 0.5176,  ..., 0.7333, 0.6627, 0.6588]]]]), 'image_path': ['/blue/pinaki.sarder/manojkumargalla/PostProcess/data/WSI_tiles/S-1905-018731_PAS_2of2/13312x_09728y.png']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation:   0%|          | 0/548 [00:41<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512])\n",
      "13312x_09728y.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'folder_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(args\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     96\u001b[0m     hp_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(f\u001b[38;5;241m.\u001b[39mreadlines())\n\u001b[0;32m---> 98\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 79\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(hp, name)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_name)\n\u001b[1;32m     78\u001b[0m img \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#torch.Size([3,28,28]\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m save_image(img, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/pred_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'folder_name' is not defined"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "notebook_dir = os.getcwd()\n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "utils_dir = os.path.abspath(os.path.join(notebook_dir, '..', 'utils'))\n",
    "if utils_dir not in sys.path:\n",
    "    sys.path.append(utils_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", (UserWarning, FutureWarning))\n",
    "from hparams import HParam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import dataloader\n",
    "import metrics\n",
    "from core.res_unet import ResUnet\n",
    "from core.res_unet_plus import ResUnetPlusPlus\n",
    "from logger import MyWriter\n",
    "import torch\n",
    "import argparse\n",
    "import glob\n",
    "import skimage\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def main(hp, name):\n",
    "    output_dir = hp.pred\n",
    "    os.makedirs(\"{}/{}\".format(hp.log, name), exist_ok=True)\n",
    "    writer = MyWriter(\"{}/{}\".format(hp.log, name))\n",
    "    \n",
    "    if hp.RESNET_PLUS_PLUS:\n",
    "        model = ResUnetPlusPlus(3).cuda()\n",
    "    else:\n",
    "        model = ResUnet(3, 64).cuda()\n",
    "\n",
    "    checkpoint = torch.load(hp.checkpoints)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\".format(hp.checkpoints, checkpoint[\"epoch\"]))\n",
    "    \n",
    "    folder_names = [name for name in os.listdir(hp.valid) if os.path.isdir(os.path.join(hp.valid, name))]\n",
    "    print(folder_names)\n",
    "    print (f'There are {len(folder_names)} image folders')\n",
    "    for foldername in folder_names:\n",
    "        output_dir = hp.pred\n",
    "        output_dir = os.path.join(output_dir, foldername)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        mass_dataset_val = dataloader.ImageDataset(\n",
    "            foldername, hp, False, transform=transforms.Compose([dataloader.ToTensorTarget()])\n",
    "        )\n",
    "        print(f'working on {foldername}')\n",
    "        # print(os.path.join(mass_dataset_val.path, \"*.jpg\"))\n",
    "        # image_list = glob.glob(\n",
    "        #         os.path.join(mass_dataset_val.path, \"*.jpg\"), recursive=True\n",
    "        #     )\n",
    "        # print(len(image_list))\n",
    "        # # Debugging print statements\n",
    "        # print(f\"Validation path: {hp.valid}\")\n",
    "        # print(f\"Number of validation images: {len(mass_dataset_val.image_list)}\")\n",
    "\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            mass_dataset_val, batch_size=1, num_workers=2, shuffle=False\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        print(type(val_dataloader))\n",
    "\n",
    "        for idx, data in enumerate(tqdm(val_dataloader, desc=\"validation\")):\n",
    "            print(data)\n",
    "            inputs = data[\"sat_img\"].cuda()\n",
    "            prob_map = model(inputs) # last activation was a sigmoid\n",
    "            outputs = (prob_map > 0.3).float()\n",
    "            print(outputs.shape)\n",
    "            image_name = mass_dataset_val.getPath(idx).split(\"/\")[-1]\n",
    "            print(image_name)\n",
    "            img = outputs[0] #torch.Size([3,28,28]\n",
    "            save_image(img, f'{output_dir}/pred_{foldername}_{idx}_{image_name}')\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    args = {\n",
    "        \"name\": \"ResUnetTrain3\",\n",
    "        \"config\": \"/home/manojkumargalla/PostProcess/config/default3.yaml\",\n",
    "        \"resume\": \"\"\n",
    "    }\n",
    "    \n",
    "    class Struct:\n",
    "        def __init__(self, entries):\n",
    "            self.__dict__.update(entries)\n",
    "            \n",
    "    args = Struct(args)\n",
    "    \n",
    "    hp = HParam(args.config)\n",
    "    with open(args.config, \"r\") as f:\n",
    "        hp_str = \"\".join(f.readlines())\n",
    "\n",
    "    main(hp, name=args.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.hparams import HParam\n",
    "from dataset.dataloader import ImageDataset\n",
    "\n",
    "hp = HParam('configs/default3.yaml')\n",
    "mass_dataset_val = ImageDataset(hp, train=False, transform=None)\n",
    "\n",
    "print(f\"Validation path: {hp.valid}\")\n",
    "print(f\"Number of validation images in dataset: {len(mass_dataset_val.image_list)}\")\n",
    "print(f\"Number of validation images using __len__: {len(mass_dataset_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mass_dataset_val.image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resn",
   "language": "python",
   "name": "resn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
